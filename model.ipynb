{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import gc\n",
    "from datetime import datetime\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import PreTrainedModel, AutoConfig\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import dataset\n",
    "\n",
    "# DEVICE = torch.device(\"cpu\")\n",
    "DEVICE = torch.device(\"cuda:0\")\n",
    "\n",
    "# pretrained_model_name = \"DeepPavlov/distilrubert-tiny-cased-conversational-v1\"\n",
    "# pretrained_model_name = \"cointegrated/rubert-tiny\"\n",
    "pretrained_model_name = \"cointegrated/rubert-tiny2\"\n",
    "# pretrained_model_name = \"DeepPavlov/rubert-base-cased\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "allowed_labels = list(set(Path(\"./allowedLabels.txt\").read_text().lower().split(\"\\n\")))\n",
    "\n",
    "labels_encoder = preprocessing.LabelEncoder()\n",
    "labels_encoder.fit(allowed_labels)\n",
    "\n",
    "orig_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = pd.read_csv(\"../columns.csv\", sep=\"<\")[\"column\"].astype(str).to_list()\n",
    "tokenizer = orig_tokenizer.train_new_from_iterator([columns], 90000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(dataset)\n",
    "\n",
    "paths = list(Path(\"../filteredData2\").glob(\"./*/*.csv\"))\n",
    "train_paths, val_paths = train_test_split(paths, test_size=0.20,\n",
    "                                          random_state=42)\n",
    "\n",
    "train_dataloader = dataset.Tables(train_paths, tokenizer, labels_encoder, use_rand=True).create_dataloader(\n",
    "    batch_size=80,\n",
    "    shuffle=True)\n",
    "val_dataloader = dataset.Tables(\n",
    "    val_paths, tokenizer, labels_encoder).create_dataloader(batch_size=100)\n",
    "\n",
    "columns_dataloader = dataset.Columns(\"../columns.csv\", tokenizer, labels_encoder).create_dataloader(batch_size=50, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "class Model(PreTrainedModel):\n",
    "    def __init__(self, config, labels_number):\n",
    "        super().__init__(config)\n",
    "        self.labels_number = labels_number\n",
    "        self.bert = AutoModel.from_pretrained(pretrained_model_name)\n",
    "        self.bert.resize_token_embeddings(tokenizer.vocab_size)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.linear = nn.Linear(self.bert.config.hidden_size, labels_number)\n",
    "        self.linear.weight.data.uniform_(0.0, 1.0)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        output = self.bert(input_ids=input_ids,\n",
    "                           return_dict=False)[0]\n",
    "        output = self.dropout(output)\n",
    "        output = self.tanh(output)\n",
    "        output = self.linear(output)\n",
    "        output = output.squeeze(0)\n",
    "\n",
    "        if len(output.shape) == 2:\n",
    "            output = output.unsqueeze(0)\n",
    "\n",
    "        cls_ids = torch.nonzero(input_ids == tokenizer.cls_token_id)\n",
    "        filtered_logits = torch.zeros(cls_ids.shape[0], output.shape[2])\n",
    "\n",
    "        for n in range(cls_ids.shape[0]):\n",
    "            i, j = cls_ids[n]\n",
    "            filtered_logits[n] = output[i, j, :]\n",
    "\n",
    "        return filtered_logits\n",
    "\n",
    "\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "\n",
    "config.update({\"hidden_dropout_prob\": 0.2,\n",
    "               \"layer_norm_eps\": 1e-7})\n",
    "\n",
    "model = Model(config, labels_number=len(allowed_labels)).to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def loss_fn(logits, targets):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    return criterion(logits, targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, loss_eval, optimizer, scheduler):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for batch, idx in zip(tqdm(data_loader), range(len(data_loader))):\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        targets = batch[\"labels\"]\n",
    "        outputs = model(input_ids=input_ids)\n",
    "        loss = loss_eval(outputs, targets)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "            print(np.mean(losses), loss.item())\n",
    "    scheduler.step()\n",
    "    return np.mean(losses)\n",
    "\n",
    "\n",
    "def eval_model(model, data_loader, loss_eval):\n",
    "    model.eval()\n",
    "\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader):\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            targets = batch[\"labels\"]\n",
    "            outputs = model(input_ids=input_ids)\n",
    "            loss = loss_eval(outputs, targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            targets = targets.cpu()\n",
    "            true_labels += list(targets[targets != -1])\n",
    "            predicted_labels += nn.Softmax(dim=1)(outputs.cpu()).argmax(\n",
    "                dim=1).tolist()\n",
    "\n",
    "\n",
    "    return np.mean(losses), f1_score(true_labels, predicted_labels, average='macro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18d7a03dac374feeb718937d4ba6b4d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Val loss: 7.6468\n",
      "F1 macro: 0.0012\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_loss, f1_macro = eval_model(model,\n",
    "                      val_dataloader,\n",
    "                      loss_fn)\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(f'Val loss: {round(val_loss, 4)}')\n",
    "print(f'F1 macro: {round(f1_macro, 4)}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = AdamW(model.parameters(), lr=1e-6, weight_decay=1e-4, eps=1e-8)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c64415639d0a43bd81f6e89ed9ab8759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2606 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.707839488983154 6.707839488983154\n",
      "6.8604806720620335 6.489990711212158\n",
      "6.540790541255059 5.645623207092285\n",
      "6.280478862432942 5.433619976043701\n",
      "6.062109779538656 5.1800360679626465\n",
      "5.879879216234127 3.914008378982544\n",
      "5.728255384178606 4.6069536209106445\n",
      "5.587062860861654 4.717736721038818\n",
      "5.464546987924088 4.528722763061523\n",
      "5.348521379995822 4.1830854415893555\n",
      "5.246956463460322 3.6162164211273193\n",
      "5.145872149870247 4.555241584777832\n",
      "5.064379196182873 5.033581733703613\n",
      "4.986481521974428 3.9801628589630127\n",
      "4.910168980462989 4.26601505279541\n",
      "4.845529437462224 3.9654064178466797\n",
      "4.780593445269783 3.710418701171875\n",
      "4.720501804828364 3.855602502822876\n",
      "4.663536426161873 3.799208164215088\n",
      "4.607157821093403 3.187418222427368\n",
      "4.556068330809571 3.729335069656372\n",
      "4.505960246258154 3.5307412147521973\n",
      "4.460263527073355 3.370115041732788\n",
      "4.413795658320461 3.5727264881134033\n",
      "4.3706481054592805 3.6931838989257812\n",
      "4.327967568713634 2.8654513359069824\n",
      "4.2907138421690405 3.5085361003875732\n",
      "Train loss: 4.2889\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b5fe4e87104d6fb28316a8dee1236a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 4.1717\n",
      "F1 macro: 0.0125\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from collections import defaultdict\n",
    "\n",
    "# history = defaultdict(list)\n",
    "\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# for epoch in range(1):\n",
    "#     print(f'Epoch: {epoch + 1}')\n",
    "#     print('-' * 10)\n",
    "#     # TRAIN\n",
    "#     train_loss = train_epoch(model,\n",
    "#                              columns_dataloader,\n",
    "#                              loss_fn,\n",
    "#                              optimizer,\n",
    "#                              scheduler)\n",
    "\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "#     print(f'Train loss: {round(train_loss, 4)}\\n')\n",
    "#     torch.save(model.state_dict(), f'./checkpoints/{datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")}')\n",
    "#     # -----------------------------------------------------\n",
    "#     #     EVAL\n",
    "#     val_loss, f1_macro = eval_model(model,\n",
    "#                           val_dataloader,\n",
    "#                           loss_fn)\n",
    "\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "#     print(f'Val loss: {round(val_loss, 4)}')\n",
    "#     print(f'F1 macro: {round(f1_macro, 4)}\\n')\n",
    "#     history['val_loss'].append(val_loss)\n",
    "\n",
    "#     # ------------------------------------------------------\n",
    "#     history['train_loss'].append(train_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for param in model.bert.parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4, eps=1e-8)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f26e274f1e4aeaafe70e03ae5c953b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/489 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "8.463425636291504 8.463425636291504\n",
      "3.1125778644391806 1.8406994342803955\n",
      "2.302556783702243 1.1832391023635864\n",
      "1.8777047603629355 0.7698885202407837\n",
      "1.6142128251883157 0.512525200843811\n",
      "Train loss: 1.4451\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bb0f79211f347e8a56d3748ca9bcb62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Val loss: 0.5492\n",
      "F1 macro: 0.4375\n",
      "\n",
      "Epoch: 2\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97d5772089b4441e942d006ed1c2fc85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/489 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "0.5614535212516785 0.5614535212516785\n",
      "0.4608249482837054 0.41856393218040466\n",
      "0.4215314002772469 0.32090625166893005\n",
      "0.40052660596727135 0.28796881437301636\n",
      "0.3791179072083975 0.2633626461029053\n",
      "Train loss: 0.3681\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a2c02f7f1da4393a68eb3f759fd30d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Val loss: 0.2753\n",
      "F1 macro: 0.6471\n",
      "\n",
      "Epoch: 3\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eab34d29cea40a991c5fe0943513c79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/489 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "0.22877277433872223 0.22877277433872223\n",
      "0.19184327490701533 0.16863662004470825\n",
      "0.18310262896676563 0.16169458627700806\n",
      "0.18210331865422352 0.21398389339447021\n",
      "0.1794204004201806 0.18831336498260498\n",
      "Train loss: 0.1763\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97081b9f2278440c9c6fc5012071962b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Val loss: 0.2185\n",
      "F1 macro: 0.7634\n",
      "\n",
      "Epoch: 4\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27371b07bbd5496f96eb17777cc5e1af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/489 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "0.127446249127388 0.127446249127388\n",
      "0.11119942344946436 0.08212170004844666\n",
      "0.10664195607217093 0.051933035254478455\n",
      "0.10376864614156987 0.21428291499614716\n",
      "0.10230658192215418 0.04335680603981018\n",
      "Train loss: 0.1028\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdd2781f46aa42f2a7ce299050f8a616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Val loss: 0.2169\n",
      "F1 macro: 0.8167\n",
      "\n",
      "Epoch: 5\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f9db77f4de54a1cb86a60f537b5ce8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/489 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "0.08745516836643219 0.08745516836643219\n",
      "0.07133479608167516 0.1062089055776596\n",
      "0.07203487473173965 0.0770566463470459\n",
      "0.06728235252690425 0.037867702543735504\n",
      "0.06796358617840154 0.041149210184812546\n",
      "Train loss: 0.0673\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a73b84f091364b9a9489904dd11962b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Val loss: 0.2015\n",
      "F1 macro: 0.8594\n",
      "\n",
      "Epoch: 6\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4617133f4659476fa54bcbecae72ee1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/489 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "0.032790251076221466 0.032790251076221466\n",
      "0.052680370185782414 0.04514368623495102\n",
      "0.051887175707095556 0.029131295159459114\n",
      "0.05076744633864463 0.010911066085100174\n",
      "0.050664014428879536 0.009743510745465755\n",
      "Train loss: 0.0498\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1707310dd2a343ada6291054281c4c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Val loss: 0.2011\n",
      "F1 macro: 0.8636\n",
      "\n",
      "Epoch: 7\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be1139c83bf48ec8c68b8f294f98cf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/489 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "0.03381166234612465 0.03381166234612465\n",
      "0.04435122543761488 0.031048865988850594\n",
      "0.040258183993111867 0.04529630020260811\n",
      "0.038529433004490014 0.015459532849490643\n",
      "0.03643706762998312 0.029008954763412476\n",
      "Train loss: 0.0368\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3435a086cc34468181898becb48510fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Val loss: 0.2005\n",
      "F1 macro: 0.8803\n",
      "\n",
      "Epoch: 8\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ebb489209d6405ea49428e11383ae47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/489 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "0.012739848345518112 0.012739848345518112\n",
      "0.03539957270420405 0.014473733492195606\n",
      "0.03351301033674178 0.026404788717627525\n",
      "0.031730751495574756 0.0011600067373365164\n",
      "0.031104278572030283 0.0207356009632349\n",
      "Train loss: 0.0305\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e1b833255de470cb8c2f9766bbc9aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Val loss: 0.2003\n",
      "F1 macro: 0.8901\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "history = defaultdict(list)\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for epoch in range(8):\n",
    "    print(f'Epoch: {epoch + 1}')\n",
    "    print('-' * 10)\n",
    "    # TRAIN\n",
    "    train_loss = train_epoch(model,\n",
    "                             train_dataloader,\n",
    "                             loss_fn,\n",
    "                             optimizer,\n",
    "                             scheduler)\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f'Train loss: {round(train_loss, 4)}\\n')\n",
    "    torch.save(model.state_dict(), f'./checkpoints/{datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")}')\n",
    "    # -----------------------------------------------------\n",
    "    #     EVAL\n",
    "    val_loss, f1_macro = eval_model(model,\n",
    "                          val_dataloader,\n",
    "                          loss_fn)\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f'Val loss: {round(val_loss, 4)}')\n",
    "    print(f'F1 macro: {round(f1_macro, 4)}\\n')\n",
    "    history['val_loss'].append(val_loss)\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    history['train_loss'].append(train_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a0bf8dabc784187a335d671dcff5409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/489 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "0.016507921740412712 0.016507921740412712\n",
      "0.02641747104543455 0.0489533431828022\n",
      "0.02447485111871577 0.007935930974781513\n",
      "0.024369739701095783 0.058784909546375275\n",
      "0.026427221185170078 0.01152932271361351\n",
      "Train loss: 0.026\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e24e5ae012cb4c34848ad92479614593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Val loss: 0.2074\n",
      "F1 macro: 0.8914\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "history = defaultdict(list)\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for epoch in range(1):\n",
    "    print(f'Epoch: {epoch + 1}')\n",
    "    print('-' * 10)\n",
    "    # TRAIN\n",
    "    train_loss = train_epoch(model,\n",
    "                             train_dataloader,\n",
    "                             loss_fn,\n",
    "                             optimizer,\n",
    "                             scheduler)\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f'Train loss: {round(train_loss, 3)}\\n')\n",
    "    torch.save(model.state_dict(), f'./checkpoints/{datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")}')\n",
    "    # -----------------------------------------------------\n",
    "    #     EVAL\n",
    "    val_loss, f1_macro = eval_model(model,\n",
    "                          val_dataloader,\n",
    "                          loss_fn)\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f'Val loss: {round(val_loss, 4)}')\n",
    "    print(f'F1 macro: {round(f1_macro, 4)}\\n')\n",
    "    history['val_loss'].append(val_loss)\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    history['train_loss'].append(train_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7099\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasLtMatmul( ltHandle, computeDesc.descriptor(), &alpha_val, mat1_ptr, Adesc.descriptor(), mat2_ptr, Bdesc.descriptor(), &beta_val, result_ptr, Cdesc.descriptor(), result_ptr, Cdesc.descriptor(), &heuristicResult.algo, workspace.data_ptr(), workspaceSize, at::cuda::getCurrentCUDAStream())`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 46\u001b[0m\n\u001b[1;32m     42\u001b[0m tokens \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([tokens])\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[1;32m     44\u001b[0m \u001b[39m# print(tokens)\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m result \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSoftmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)(model(tokens))\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     48\u001b[0m predicted_labels \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m     49\u001b[0m true_labels \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(labels)\n",
      "File \u001b[0;32m/mnt/sdb1/university/thesis/ru-tables/rudoduo/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_ids):\n\u001b[0;32m---> 13\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m     14\u001b[0m                        return_dict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     15\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(output)\n\u001b[1;32m     16\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtanh(output)\n",
      "File \u001b[0;32m/mnt/sdb1/university/thesis/ru-tables/rudoduo/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/sdb1/university/thesis/ru-tables/rudoduo/venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1019\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1010\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1012\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   1013\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1014\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1017\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1018\u001b[0m )\n\u001b[0;32m-> 1019\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1020\u001b[0m     embedding_output,\n\u001b[1;32m   1021\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1022\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1023\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1024\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1025\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1026\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1027\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1028\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1029\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1030\u001b[0m )\n\u001b[1;32m   1031\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1032\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/sdb1/university/thesis/ru-tables/rudoduo/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/sdb1/university/thesis/ru-tables/rudoduo/venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:609\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    600\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    601\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    602\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    606\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    607\u001b[0m     )\n\u001b[1;32m    608\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 609\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    610\u001b[0m         hidden_states,\n\u001b[1;32m    611\u001b[0m         attention_mask,\n\u001b[1;32m    612\u001b[0m         layer_head_mask,\n\u001b[1;32m    613\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    614\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    615\u001b[0m         past_key_value,\n\u001b[1;32m    616\u001b[0m         output_attentions,\n\u001b[1;32m    617\u001b[0m     )\n\u001b[1;32m    619\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    620\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/mnt/sdb1/university/thesis/ru-tables/rudoduo/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/sdb1/university/thesis/ru-tables/rudoduo/venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:495\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    484\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    485\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    492\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    493\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    494\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 495\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    496\u001b[0m         hidden_states,\n\u001b[1;32m    497\u001b[0m         attention_mask,\n\u001b[1;32m    498\u001b[0m         head_mask,\n\u001b[1;32m    499\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    500\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    501\u001b[0m     )\n\u001b[1;32m    502\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    504\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/sdb1/university/thesis/ru-tables/rudoduo/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/sdb1/university/thesis/ru-tables/rudoduo/venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:425\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    416\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    417\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    423\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    424\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 425\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    426\u001b[0m         hidden_states,\n\u001b[1;32m    427\u001b[0m         attention_mask,\n\u001b[1;32m    428\u001b[0m         head_mask,\n\u001b[1;32m    429\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    430\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    431\u001b[0m         past_key_value,\n\u001b[1;32m    432\u001b[0m         output_attentions,\n\u001b[1;32m    433\u001b[0m     )\n\u001b[1;32m    434\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    435\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/sdb1/university/thesis/ru-tables/rudoduo/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/sdb1/university/thesis/ru-tables/rudoduo/venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:284\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    275\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    276\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    282\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    283\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 284\u001b[0m     mixed_query_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mquery(hidden_states)\n\u001b[1;32m    286\u001b[0m     \u001b[39m# If this is instantiated as a cross-attention module, the keys\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39m# and values come from an encoder; the attention mask needs to be\u001b[39;00m\n\u001b[1;32m    288\u001b[0m     \u001b[39m# such that the encoder's padding tokens are not attended to.\u001b[39;00m\n\u001b[1;32m    289\u001b[0m     is_cross_attention \u001b[39m=\u001b[39m encoder_hidden_states \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/sdb1/university/thesis/ru-tables/rudoduo/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/sdb1/university/thesis/ru-tables/rudoduo/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasLtMatmul( ltHandle, computeDesc.descriptor(), &alpha_val, mat1_ptr, Adesc.descriptor(), mat2_ptr, Bdesc.descriptor(), &beta_val, result_ptr, Cdesc.descriptor(), result_ptr, Cdesc.descriptor(), &heuristicResult.algo, workspace.data_ptr(), workspaceSize, at::cuda::getCurrentCUDAStream())`"
     ]
    }
   ],
   "source": [
    "\n",
    "MAX_TOKENS_PER_COLUMN = 200  # 2 of those for CLS and SEP\n",
    "MAX_COLUMNS = 6\n",
    "MAX_TOKENS = 200  # per table\n",
    "\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "\n",
    "\n",
    "num = np.random.randint(0, len(val_paths))\n",
    "print(num)\n",
    "\n",
    "df = pd.read_csv(val_paths[num], sep=\"|\")\n",
    "tokens = []\n",
    "with open(val_paths[num]) as file:\n",
    "    labels = file.readline().lower().rstrip('\\n').split(\"|\")\n",
    "\n",
    "# print(labels)\n",
    "assert len(labels) == len(df.columns)\n",
    "\n",
    "columns = df.columns[:MAX_COLUMNS]\n",
    "labels = labels[:MAX_COLUMNS]\n",
    "\n",
    "tokens_per_column = min(\n",
    "    MAX_TOKENS // len(labels), MAX_TOKENS_PER_COLUMN)\n",
    "\n",
    "for label, _ in zip(df.columns, range(MAX_COLUMNS)):\n",
    "    str_repr_of_column = df[label].astype(str).str.cat(sep=\" \")\n",
    "    tokens += tokenizer(str_repr_of_column, truncation=True,\n",
    "                                max_length=tokens_per_column).input_ids\n",
    "\n",
    "labels = labels_encoder.transform(labels)[:MAX_COLUMNS]\n",
    "\n",
    "# for label, _ in zip(df.columns, range(MAX_COLUMNS)):\n",
    "#     str_repr_of_column = df[label].astype(str).str.cat(sep=\" \")[:40]\n",
    "#     tokens += tokenizer(str_repr_of_column, truncation=True,\n",
    "#                         padding='max_length', max_length=10).input_ids\n",
    "\n",
    "# tokens = [np.pad(tokens, (0, MAX_TOKENS), 'constant')[:MAX_TOKENS]]\n",
    "# labels = labels_encoder.transform(labels)\n",
    "\n",
    "tokens = torch.tensor([tokens]).to(DEVICE)\n",
    "\n",
    "# print(tokens)\n",
    "\n",
    "result = nn.Softmax(dim=1)(model(tokens)).argmax(dim=1)\n",
    "\n",
    "predicted_labels += result.tolist()\n",
    "true_labels += list(labels)\n",
    "print(\"pred:\", labels_encoder.inverse_transform(result))\n",
    "print(\"true:\", labels_encoder.inverse_transform(labels))\n",
    "# print(model(tokens).shape)\n",
    "#\n",
    "\n",
    "\n",
    "# print(tokens)\n",
    "\n",
    "# print(model(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18b131874f2b466598cd0ce5bea0c614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def find_problematic_labels(model, data_loader):\n",
    "    model.eval()\n",
    "\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader):\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            targets = batch[\"labels\"]\n",
    "            outputs = model(input_ids=input_ids)\n",
    "\n",
    "            targets = targets.cpu()\n",
    "            true_labels += list(targets[targets != -1])\n",
    "            predicted_labels += nn.Softmax(dim=1)(outputs.cpu()).argmax(\n",
    "                dim=1).tolist()\n",
    "\n",
    "\n",
    "    c = Counter()\n",
    "    for x, y in zip(list(true_labels), predicted_labels):\n",
    "        if x.item() != y:\n",
    "            c[( labels_encoder.inverse_transform([x.item()])[0],  labels_encoder.inverse_transform([y])[0])] += 1\n",
    "\n",
    "    return c\n",
    "\n",
    "counter = find_problematic_labels(model, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('', ''), 48),\n",
       " (('', ''), 27),\n",
       " (('', ''), 26),\n",
       " (('', ''), 25),\n",
       " (('', ''), 25),\n",
       " (('', ''), 23),\n",
       " (('', ''), 20),\n",
       " (('', ''), 19),\n",
       " (('', ''), 19),\n",
       " (('', ' '), 17),\n",
       " (('', ''), 15),\n",
       " (('', ''), 15),\n",
       " (('', ''), 14),\n",
       " (('', ''), 13),\n",
       " (('', ''), 12),\n",
       " (('', ''), 12),\n",
       " (('', ''), 11),\n",
       " (('', ''), 11),\n",
       " (('', ''), 11),\n",
       " (('', ''), 11),\n",
       " (('', ''), 11),\n",
       " (('', ''), 9),\n",
       " (('', ''), 9),\n",
       " (('', ', '), 8),\n",
       " (('', ''), 8),\n",
       " (('', ''), 8),\n",
       " (('', ''), 8),\n",
       " (('', ''), 8),\n",
       " (('', ''), 7),\n",
       " (('', ''), 7)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.most_common(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01cb9e0d05ca4232993b13dc3c22cd2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/435 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_counter = find_problematic_labels(model, train_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('', ''), 84),\n",
       " (('', ''), 56),\n",
       " (('', ''), 51),\n",
       " (('', ''), 48),\n",
       " (('', ''), 45),\n",
       " (('', ''), 40),\n",
       " (('', ''), 27),\n",
       " (('', ''), 25),\n",
       " (('', ''), 24),\n",
       " (('', ''), 24),\n",
       " (('', ''), 23),\n",
       " (('', ''), 18),\n",
       " (('', ''), 16),\n",
       " (('', ''), 16),\n",
       " (('', ''), 14),\n",
       " (('', ''), 14),\n",
       " (('', ''), 14),\n",
       " (('', ''), 13),\n",
       " (('', ''), 12),\n",
       " (('', ''), 12),\n",
       " (('', ''), 11),\n",
       " (('', ''), 11),\n",
       " (('', ''), 11),\n",
       " (('', ''), 11),\n",
       " (('', ''), 10),\n",
       " (('', ''), 10),\n",
       " (('', ''), 10),\n",
       " (('', ''), 10),\n",
       " (('', ''), 10),\n",
       " (('', ' '), 10)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_counter.most_common(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../filteredData2/4312554/table_12.csv\n"
     ]
    }
   ],
   "source": [
    "#  print(val_paths[9616])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5388\n",
      "['', '', '', '', '']\n",
      "26 24 25 26 24 23 21 19 25 27 20 24 22 30\n",
      "['26', '24', '25', '26', '24', '23', '21', '19', '25', '27', '20', '24', '22', '30']\n",
      "5395\n",
      "['', '']\n",
      "16 17 17 18 18 18 18 19 19 19 19 19 19 19 19 19 19 29 20 20\n",
      "['16', '17', '17', '18', '18', '18', '18', '19', '19', '19', '19', '19', '19', '19', '19', '19', '19', '29', '20', '20']\n",
      "8936\n",
      "['', '', '']\n",
      "8  23  29 \n",
      "['8', '', '23', '', '29', '']\n",
      "9132\n",
      "['', '']\n",
      "29 50 35 21 11 44 24 41 17 38 51 45 35 24 20 21 50 31 58 22 28 20 38 38 31 50 38 49 ? 43 28 49 47 55 41 23 32 28\n",
      "['29', '50', '35', '21', '11', '44', '24', '41', '17', '38', '51', '45', '35', '24', '20', '21', '50', '31', '58', '22', '28', '20', '38', '38', '31', '50', '38', '49']\n",
      "9142\n",
      "['', '', ' ', '']\n",
      "22 26\n",
      "['22', '26']\n",
      "9448\n",
      "['', '', '', '']\n",
      "20 19 22 22 19 22 19 19 22 22 18 21 20 20 21 23 22 18 19 21 21 23 20 22 18 21 23 23 19 20 20 21 21 20 21 21 21 21 19 21 20 22 22 22 20 20 18 19 20 18\n",
      "['20', '19', '22', '22', '19', '22', '19', '19', '22', '22', '18', '21', '20', '20', '21', '23', '22', '18', '19', '21', '21', '23', '20', '22', '18', '21', '23', '23']\n",
      "9771\n",
      "['', '', '', '']\n",
      "37  23  21  15 , 4  15 , 1  14 , 6 \n",
      "['37', '', '23', '', '21', '', '15', '', ',', '4', '', '15', '', ',', '1', '', '14', '', ',', '6', '']\n"
     ]
    }
   ],
   "source": [
    "for num in range(0, len(val_paths)):\n",
    "\n",
    "    df = pd.read_csv(val_paths[num], sep=\"|\")\n",
    "\n",
    "    tokens = []\n",
    "    with open(val_paths[num]) as file:\n",
    "        labels = file.readline().lower().rstrip('\\n').split(\"|\")\n",
    "\n",
    "    if '' in labels:\n",
    "        print(num)\n",
    "        print(labels)\n",
    "        # print(df.head())\n",
    "\n",
    "        str_repr_of_column = df[''].astype(str).str.cat(sep=\" \")[:200]\n",
    "        print(str_repr_of_column)\n",
    "        print(tokenizer.tokenize(str_repr_of_column, truncation=True, max_length=28))\n",
    "\n",
    "\n",
    "# print(val_paths[20])\n",
    "# df = pd.read_csv(val_paths[20], sep=\"|\")\n",
    "#\n",
    "# print(df.head())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(tokenizer.tokenize(\"9.4 1.1 1.0 1:1\", truncation=True, max_length=28))\n",
    "\n",
    "# assert len(labels) == len(df.columns)\n",
    "#\n",
    "# columns = df.columns[:MAX_COLUMNS]\n",
    "# labels = labels[:MAX_COLUMNS]\n",
    "#\n",
    "#\n",
    "#\n",
    "# TOKENS_PER_COLUMN = MAX_TOKENS // len(labels)\n",
    "#\n",
    "# for label, _ in zip(df.columns, range(MAX_COLUMNS)):\n",
    "#     str_repr_of_column = df[label].astype(str).str.cat(sep=\" \")[:200]\n",
    "#     tokens += tokenizer(str_repr_of_column, truncation=True,\n",
    "#                         max_length=TOKENS_PER_COLUMN).input_ids\n",
    "#\n",
    "# labels = labels_encoder.transform(labels)[:MAX_COLUMNS]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cb5e5f5c0e39f2b6d47a356783d500f753405b03bf9c5b9d3f224522e546d407"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
